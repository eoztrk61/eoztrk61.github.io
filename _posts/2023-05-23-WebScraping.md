# Web Scraping & OpenRefine 
If we start with the definition, the scraping process is to obtain data by using the limited authorisations in part left open to the user and to obtain more critical information by processing this raw data. 
How do we scrape the web? A URL is needed for web scraping. A request is made to the URL and the response is analysed to extract the searched data. Requesting and displaying is basically a web browser behaviour. Web browsing is the primary content of web scraping. Many operations such as searching, filtering, detailing, parsing, reformatting, editing, and copying can be performed on the return from the server. One of the web scraping programmes that we can use to perform these operations is **[OpenRefine](https://openrefine.org/)**. 

I had never heard of OpenRefine before, but in the class today I realised that it is a very useful program, especially for obtaining and compiling large amounts of data from websites. The main features of OpenRefine are as follows:
`+` _Free and open source

`+` Data cleaning and filtering

`+` Import data from various formats

`+` Data reconciliation and matching

`+` Ability to enrich data through APIs

`+` Data transformation

`+` Linking data

`+` Advanced-Data Operations_

Of course, it is not possible for me to memorise the codes required for the use and transformation of the data. Since the lecturer will upload the necessary codes and steps to Moodle, the programme will be even easier to use. It looks like I will use OpenRefine in the final project.
